import os
import cloudscraper
import requests
from bs4 import BeautifulSoup
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, CallbackQueryHandler, ContextTypes
from flask import Flask
import threading
import logging

# 专转  
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# 专转
TOKEN = os.getenv("TELEGRAM_TOKEN")
if not TOKEN:
    logger.error("砖: 拽  专!")
    exit(1)

NEWS_SITES = {
    'ynet': 'https://www.ynet.co.il/news',
    'arutz7': 'https://www.inn.co.il/api/NewAPI/Cat?type=10',
    'walla': 'https://news.walla.co.il/',
    'sport5': 'https://m.sport5.co.il/',
    'sport1': 'https://sport1.maariv.co.il/',
    'one': 'https://m.one.co.il/mobile/'
}

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124'
}

# 爪专转 驻拽爪转 Flask
app = Flask(__name__)

def run_flask():
    port = int(os.environ.get("PORT", 8080))
    app.run(host="0.0.0.0", port=port)

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("专 ! 砖转砖 -/latest 拽.")

def scrape_ynet():
    try:
        scraper = cloudscraper.create_scraper()
        soup = BeautifulSoup(scraper.get(NEWS_SITES['ynet'], headers=HEADERS).text, 'html.parser')
        return [{'title': item.text.strip(), 'link': item.find('a')['href']} for item in soup.select('div.slotTitle')[:5]]
    except Exception as e:
        logger.error(f"砖 -Ynet: {e}")
        return []

def scrape_arutz7():
    try:
        response = requests.get(NEWS_SITES['arutz7'], headers=HEADERS)
        response.raise_for_status()
        data = response.json()
        items = data.get('Items', []) if 'Items' in data else data
        return [
            {
                'time': item.get('time', item.get('itemDate', " 砖注")[:16].replace('T', ' ')),
                'title': item.get('title', ' 转专转'),
                'link': item.get('shotedLink', item.get('link', '#'))
            } for item in items[:3]
        ]
    except Exception as e:
        logger.error(f"砖 注专抓 7: {e}")
        return []

def scrape_walla():
    try:
        scraper = cloudscraper.create_scraper()
        soup = BeautifulSoup(scraper.get(NEWS_SITES['walla'], headers=HEADERS).text, 'html.parser')
        items = soup.select_one('div.top-section-newsflash.no-mobile').select('a') if soup.select_one('div.top-section-newsflash.no-mobile') else []
        results = []
        for item in items:
            title = item.get_text(strip=True)
            if title in ["拽 砖转", "拽"]:
                continue
            if len(title) > 5 and title[2] == ':':
                title = title[:5] + ": " + title[5:]
            link = item['href']
            if not link.startswith('http'):
                link = f"https://news.walla.co.il{link}"
            results.append({'title': title, 'link': link})
        return results[:3]
    except Exception as e:
        logger.error(f"砖 -Walla: {e}")
        return []

def scrape_sport5():
    try:
        url = 'https://m.sport5.co.il/'
        scraper = cloudscraper.create_scraper()
        soup = BeautifulSoup(scraper.get(url, headers=HEADERS).text, 'html.parser')
        
        articles = soup.select('nav.posts-list.posts-list-articles ul li')
        
        results = []
        for item in articles[:3]:
            link_tag = item.find('a', class_='item')
            title_tag = item.find('h2', class_='post-title')
            time_tag = item.find('em', class_='time')
            
            title = title_tag.get_text(strip=True) if title_tag else ' 转专转'
            link = link_tag['href'] if link_tag else '#'
            time = time_tag.get_text(strip=True) if time_tag else ' 砖注'
            
            if link and not link.startswith('http'):
                link = f"https://m.sport5.co.il{link}"
            
            results.append({
                'time': time,
                'title': title,
                'link': link
            })
        
        logger.info(f"住拽专驻 住驻专 5 爪: {len(results)} 转转 砖驻")
        return results, None
    
    except Exception as e:
        logger.error(f"砖 住拽专驻 住驻专 5: {e}")
        return [], f"砖  注: {str(e)}"

def scrape_sport1():
    try:
        url = 'https://sport1.maariv.co.il/'
        scraper = cloudscraper.create_scraper()
        soup = BeautifulSoup(scraper.get(url, headers=HEADERS).text, 'html.parser')
        
        articles = soup.select('div.hot-news-container article.article-card')
        
        results = []
        for item in articles[:3]:
            link_tag = item.find_parent('a', class_='image-wrapper')
            title_tag = item.find('h3', class_='article-card-title')
            time_tag = item.find('time', class_='entry-date')
            
            title = title_tag.get_text(strip=True) if title_tag else ' 转专转'
            link = link_tag['href'] if link_tag else '#'
            time = time_tag.get_text(strip=True) if time_tag else ' 砖注'
            
            if link and not link.startswith('http'):
                link = f"https://sport1.maariv.co.il{link}"
            
            results.append({
                'time': time,
                'title': title,
                'link': link
            })
        
        logger.info(f"住拽专驻 住驻专 1 爪: {len(results)} 转转 砖驻")
        return results, None
    
    except Exception as e:
        logger.error(f"砖 住拽专驻 住驻专 1: {e}")
        return [], f"砖  注: {str(e)}"

def scrape_one():
    try:
        url = 'https://m.one.co.il/mobile/'
        scraper = cloudscraper.create_scraper()
        soup = BeautifulSoup(scraper.get(url, headers=HEADERS).text, 'html.parser')
        
        articles = soup.select('a.mobile-hp-article-plain')
        
        results = []
        for item in articles[:3]:
            link_tag = item
            title_tag = item.find('h1')
            #   驻专砖 拽注 , 砖转砖 " 砖注" 专专转 
            time = ' 砖注'
            
            title = title_tag.get_text(strip=True) if title_tag else ' 转专转'
            link = link_tag['href'] if link_tag else '#'
            
            if link and not link.startswith('http'):
                link = f"https://m.one.co.il{link}"
            
            results.append({
                'time': time,
                'title': title,
                'link': link
            })
        
        logger.info(f"住拽专驻 ONE 爪: {len(results)} 转转 砖驻")
        return results, None
    
    except Exception as e:
        logger.error(f"砖 住拽专驻 ONE: {e}")
        return [], f"砖  注: {str(e)}"

async def latest(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("驻砖 拽...")
    ynet_news = scrape_ynet()
    arutz7_news = scrape_arutz7()
    walla_news = scrape_walla()

    news = {'Ynet': ynet_news, '注专抓 7': arutz7_news, 'Walla': walla_news}
    message = " **拽 专** \n\n"
    for site, articles in news.items():
        message += f"**{site}:**\n"
        if articles:
            for idx, article in enumerate(articles[:3], 1):
                if 'time' in article:
                    message += f"{idx}. [{article['time']} - {article['title']}]({article['link']})\n"
                else:
                    message += f"{idx}. [{article['title']}]({article['link']})\n"
        else:
            message += " 转 注 专注\n"
        message += "\n"
    
    keyboard = [[InlineKeyboardButton("金 拽转 拽 住驻专", callback_data='sports_news')]]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await update.message.reply_text(text=message, parse_mode='Markdown', disable_web_page_preview=True, reply_markup=reply_markup)

async def sports_news(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    
    await query.message.reply_text("驻砖 拽 住驻专...")
    
    sport5_news, sport5_error = scrape_sport5()
    sport1_news, sport1_error = scrape_sport1()
    one_news, one_error = scrape_one()
    
    message = "**住驻专 5**\n"
    if sport5_news:
        for idx, article in enumerate(sport5_news[:3], 1):
            if 'time' in article:
                message += f"{idx}. {article['time']} - [{article['title']}]({article['link']})\n"
            else:
                message += f"{idx}. [{article['title']}]({article['link']})\n"
    else:
        message += " 转 爪 拽\n"
        if sport5_error:
            message += f"**驻专 砖:** {sport5_error}\n"
    
    message += "\n**住驻专 1**\n"
    if sport1_news:
        for idx, article in enumerate(sport1_news[:3], 1):
            if 'time' in article:
                message += f"{idx}. {article['time']} - [{article['title']}]({article['link']})\n"
            else:
                message += f"{idx}. [{article['title']}]({article['link']})\n"
    else:
        message += " 转 爪 拽\n"
        if sport1_error:
            message += f"**驻专 砖:** {sport1_error}\n"
    
    message += "\n**ONE**\n"
    if one_news:
        for idx, article in enumerate(one_news[:3], 1):
            if 'time' in article:
                message += f"{idx}. {article['time']} - [{article['title']}]({article['link']})\n"
            else:
                message += f"{idx}. [{article['title']}]({article['link']})\n"
    else:
        message += " 转 爪 拽\n"
        if one_error:
            message += f"**驻专 砖:** {one_error}\n"
    
    keyboard = [[InlineKeyboardButton(" 专 注 专砖", callback_data='latest_news')]]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await query.message.reply_text(text=message, parse_mode='Markdown', disable_web_page_preview=True, reply_markup=reply_markup)

async def latest_news(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    
    ynet_news = scrape_ynet()
    arutz7_news = scrape_arutz7()
    walla_news = scrape_walla()

    news = {'Ynet': ynet_news, '注专抓 7': arutz7_news, 'Walla': walla_news}
    message = " **拽 专** \n\n"
    for site, articles in news.items():
        message += f"**{site}:**\n"
        if articles:
            for idx, article in enumerate(articles[:3], 1):
                if 'time' in article:
                    message += f"{idx}. [{article['time']} - {article['title']}]({article['link']})\n"
                else:
                    message += f"{idx}. [{article['title']}]({article['link']})\n"
        else:
            message += " 转 注 专注\n"
        message += "\n"
    
    keyboard = [[InlineKeyboardButton("金 拽转 拽 住驻专", callback_data='sports_news')]]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await query.message.reply_text(text=message, parse_mode='Markdown', disable_web_page_preview=True, reply_markup=reply_markup)

@app.route('/')
def home():
    return "Bot is alive!"

if __name__ == "__main__":
    logger.info("转 转 砖专转 ...")
    
    flask_thread = threading.Thread(target=run_flask)
    flask_thread.start()
    
          logger.info(f"住 转专 专 注 拽: {TOKEN[:10]}...")
      try:
          bot_app = Application.builder().token(TOKEN).build()
          bot_app.add_handler(CommandHandler("start", start))
          bot_app.add_handler(CommandHandler("latest", latest))
          bot_app.add_handler(CallbackQueryHandler(sports_news, pattern='sports_news'))
          bot_app.add_handler(CallbackQueryHandler(latest_news, pattern='latest_news'))
          logger.info("转专转 专 爪!")
          bot_app.run_polling(allowed_updates=Update.ALL_TYPES)
      except Exception as e:
          logger.error(f"砖 专爪转 : {e}")