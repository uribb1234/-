import os
import cloudscraper
import requests
from bs4 import BeautifulSoup
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, CallbackQueryHandler, ContextTypes
from flask import Flask
import threading
import logging
from requests_html import HTMLSession
from data_logger import log_interaction, save_to_excel

# 专转  
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# 专转
TOKEN = os.getenv("TELEGRAM_TOKEN")
if not TOKEN:
    logger.error("砖: 拽  专!")
    exit(1)

NEWS_SITES = {
    'ynet': 'https://www.ynet.co.il/news',
    'arutz7': 'https://www.inn.co.il/api/NewAPI/Cat?type=10',
    'walla': 'https://news.walla.co.il/',
    'sport5': 'https://m.sport5.co.il/',
    'sport1': 'https://sport1.maariv.co.il/',
    'one': 'https://m.one.co.il/mobile/',
    'ynet_tech': 'https://www.ynet.co.il/digital/technews',
    'channel14': 'https://www.now14.co.il/'
}

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Referer': 'https://www.google.com/'
}

# 爪专转 驻拽爪转 Flask 
app = Flask(__name__)

# 爪专转 驻拽爪转 Telegram
bot_app = Application.builder().token(TOKEN).build()

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    chat = await context.bot.get_chat(user_id)
    username = chat.username
    logger.info(f"User {user_id} sent /start, username: {username}")
    log_interaction(user_id, "/start", username)
    await update.message.reply_text("专 ! 砖转砖 -/latest 拽.")

async def download(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    chat = await context.bot.get_chat(user_id)
    username = chat.username
    logger.info(f"User {user_id} sent /download, username: {username}")
    log_interaction(user_id, "/download", username)
    SECRET_PASSWORD = os.getenv("DOWNLOAD_PASSWORD")

    if not SECRET_PASSWORD:
        await update.message.reply_text("砖: 住住  专转 砖专转!")
        return
    
    if not context.args or context.args[0] != SECRET_PASSWORD:
        await update.message.reply_text("住住 砖!  砖.")
        return
    
    try:
        filename = save_to_excel()
        if not os.path.exists(filename):
            await update.message.reply_text("砖: 拽抓  爪专!")
            return
        with open(filename, 'rb') as file:
            await update.message.reply_text(" 转 砖!")
            await update.message.reply_document(document=file, filename="bot_usage.xlsx")
        os.remove(filename)
    except Exception as e:
        logger.error(f"砖 砖转 拽抓: {e}")
        await update.message.reply_text(f"砖 专: {str(e)}")

def scrape_ynet():
    try:
        scraper = cloudscraper.create_scraper()
        soup = BeautifulSoup(scraper.get(NEWS_SITES['ynet'], headers=HEADERS).text, 'html.parser')
        return [{'title': item.text.strip(), 'link': item.find('a')['href']} for item in soup.select('div.slotTitle')[:5]]
    except Exception as e:
        logger.error(f"砖 -Ynet: {e}")
        return []

def scrape_arutz7():
    try:
        response = requests.get(NEWS_SITES['arutz7'], headers=HEADERS)
        response.raise_for_status()
        data = response.json()
        items = data.get('Items', []) if 'Items' in data else data
        return [
            {
                'time': item.get('time', item.get('itemDate', " 砖注")[:16].replace('T', ' ')),
                'title': item.get('title', ' 转专转'),
                'link': item.get('shotedLink', item.get('link', '#'))
            } for item in items[:3]
        ]
    except Exception as e:
        logger.error(f"砖 注专抓 7: {e}")
        return []

def scrape_walla():
    try:
        scraper = cloudscraper.create_scraper()
        soup = BeautifulSoup(scraper.get(NEWS_SITES['walla'], headers=HEADERS).text, 'html.parser')
        items = soup.select_one('div.top-section-newsflash.no-mobile').select('a') if soup.select_one('div.top-section-newsflash.no-mobile') else []
        results = []
        for item in items:
            title = item.get_text(strip=True)
            if title in ["拽 砖转", "拽"]:
                continue
            if len(title) > 5 and title[2] == ':':
                title = title[:5] + ": " + title[5:]
            link = item['href']
            if not link.startswith('http'):
                link = f"https://news.walla.co.il{link}"
            results.append({'title': title, 'link': link})
        return results[:3]
    except Exception as e:
        logger.error(f"砖 -Walla: {e}")
        return []

def scrape_sport5():
    try:
        url = 'https://m.sport5.co.il/'
        scraper = cloudscraper.create_scraper()
        soup = BeautifulSoup(scraper.get(url, headers=HEADERS).text, 'html.parser')
        
        articles = soup.select('nav.posts-list.posts-list-articles ul li')
        
        results = []
        for item in articles[:3]:
            link_tag = item.find('a', class_='item')
            title_tag = item.find('h2', class_='post-title')
            time_tag = item.find('em', class_='time')
            
            title = title_tag.get_text(strip=True) if title_tag else ' 转专转'
            link = link_tag['href'] if link_tag else '#'
            time = time_tag.get_text(strip=True) if time_tag else ' 砖注'
            
            if link and not link.startswith('http'):
                link = f"https://m.sport5.co.il{link}"
            
            results.append({
                'time': time,
                'title': title,
                'link': link
            })
        
        logger.info(f"住拽专驻 住驻专 5 爪: {len(results)} 转转 砖驻")
        return results, None
    except Exception as e:
        logger.error(f"砖 住拽专驻 住驻专 5: {e}")
        return [], f"砖  注: {str(e)}"

def scrape_sport1():
    try:
        url = 'https://sport1.maariv.co.il/'
        scraper = cloudscraper.create_scraper()
        soup = BeautifulSoup(scraper.get(url, headers=HEADERS).text, 'html.parser')
        
        articles = soup.select('div.hot-news-container article.article-card')
        
        results = []
        for item in articles[:3]:
            link_tag = item.find_parent('a', class_='image-wrapper')
            title_tag = item.find('h3', class_='article-card-title')
            time_tag = item.find('time', class_='entry-date')
            
            title = title_tag.get_text(strip=True) if title_tag else ' 转专转'
            link = link_tag['href'] if link_tag else '#'
            time = time_tag.get_text(strip=True) if time_tag else ' 砖注'
            
            if link and not link.startswith('http'):
                link = f"https://sport1.maariv.co.il{link}"
            
            results.append({
                'time': time,
                'title': title,
                'link': link
            })
        
        logger.info(f"住拽专驻 住驻专 1 爪: {len(results)} 转转 砖驻")
        return results, None
    except Exception as e:
        logger.error(f"砖 住拽专驻 住驻专 1: {e}")
        return [], f"砖  注: {str(e)}"

def scrape_one():
    try:
        url = 'https://m.one.co.il/mobile/'
        scraper = cloudscraper.create_scraper()
        soup = BeautifulSoup(scraper.get(url, headers=HEADERS).text, 'html.parser')
        
        articles = soup.select('a.mobile-hp-article-plain')
        
        results = []
        for item in articles[:3]:
            link_tag = item
            title_tag = item.find('h1')
            time = ' 砖注'
            
            title = title_tag.get_text(strip=True) if title_tag else ' 转专转'
            link = link_tag['href'] if link_tag else '#'
            
            if link and not link.startswith('http'):
                link = f"https://m.one.co.il{link}"
            
            results.append({
                'time': time,
                'title': title,
                'link': link
            })
        
        logger.info(f"住拽专驻 ONE 爪: {len(results)} 转转 砖驻")
        return results, None
    except Exception as e:
        logger.error(f"砖 住拽专驻 ONE: {e}")
        return [], f"砖  注: {str(e)}"

def scrape_ynet_tech():
    try:
        scraper = cloudscraper.create_scraper()
        soup = BeautifulSoup(scraper.get(NEWS_SITES['ynet_tech'], headers=HEADERS, timeout=1).text, 'html.parser')
        logger.info(f"Ynet Tech HTML length: {len(soup.text)} characters")
        
        articles = soup.select('div.slotView')[:3]
        logger.info(f"Found {len(articles)} articles in Ynet Tech")
        
        results = []
        for idx, article in enumerate(articles):
            title_tag = article.select_one('div.slotTitle a')
            link_tag = title_tag
            time_tag = article.select_one('span.dateView')
            
            title = title_tag.get_text(strip=True) if title_tag else ' 转专转'
            link = link_tag['href'] if link_tag else '#'
            time = time_tag.get_text(strip=True) if time_tag else ' 砖注'
            
            if not link.startswith('http'):
                link = f"https://www.ynet.co.il{link}"
            
            results.append({
                'title': title,
                'link': link,
                'time': time
            })
            logger.info(f"Article {idx+1}: title='{title}', link='{link}', time='{time}'")
        
        if not results:
            logger.warning(" 爪 转转 -Ynet Tech")
            return [], " 爪 转转"
        
        logger.info(f"住拽专驻 Ynet Tech 爪: {len(results)} 转转 砖驻")
        return results, None
    except Exception as e:
        logger.error(f"砖 住拽专驻 Ynet Tech: {str(e)}")
        return [], f"砖  注: {str(e)}"

def scrape_channel14():
    try:
        session = HTMLSession()
        response = session.get(NEWS_SITES['channel14'], headers=HEADERS, timeout=1)
        response.html.render(timeout=5, sleep=0.5)
        
        logger.info(f"Channel 14 response status: {response.status_code}")
        logger.info(f"Channel 14 HTML length: {len(response.html.html)} characters")
        
        if response.status_code != 200:
            logger.warning(f"Channel 14 住 转 拽砖 (status: {response.status_code})")
            return [], f"砖转 {response.status_code}: 砖 住"
        
        soup = BeautifulSoup(response.html.html, 'html.parser')
        
        articles = soup.select('article.post')[:3]
        logger.info(f"Found {len(articles)} articles in Channel 14")
        
        results = []
        for idx, article in enumerate(articles):
            title_tag = article.select_one('h2.entry-title a') or article.select_one('h3 a')
            time_tag = article.select_one('time.entry-date')
            
            title = title_tag.get_text(strip=True) if title_tag else ' 转专转'
            link = title_tag['href'] if title_tag else '#'
            time = time_tag.get_text(strip=True) if time_tag else ' 砖注'
            
            if not link.startswith('http'):
                link = f"https://www.now14.co.il{link}"
            
            results.append({
                'title': title,
                'link': link,
                'time': time
            })
            logger.info(f"Article {idx+1}: title='{title}', link='{link}', time='{time}'")
        
        if not results:
            logger.warning(" 爪 转转 注专抓 14")
            return [], " 爪 转转"
        
        logger.info(f"住拽专驻 注专抓 14 爪: {len(results)} 转转 砖驻")
        return results, None
    except Exception as e:
        logger.error(f"砖 住拽专驻 注专抓 14: {str(e)}")
        return [], f"砖  注: {str(e)}"

async def latest(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    chat = await context.bot.get_chat(user_id)
    username = chat.username
    logger.info(f"User {user_id} sent /latest, username: {username}")
    log_interaction(user_id, "/latest", username)
    await update.message.reply_text("驻砖 拽...")
    ynet_news = scrape_ynet()
    arutz7_news = scrape_arutz7()
    walla_news = scrape_walla()

    news = {'Ynet': ynet_news, '注专抓 7': arutz7_news, 'Walla': walla_news}
    message = " **拽 专** \n\n"
    for site, articles in news.items():
        message += f"**{site}:**\n"
        if articles:
            for idx, article in enumerate(articles[:3], 1):
                if 'time' in article:
                    message += f"{idx}. [{article['time']} - {article['title']}]({article['link']})\n"
                else:
                    message += f"{idx}. [{article['title']}]({article['link']})\n"
        else:
            message += " 转 注 专注\n"
        message += "\n"
    
    keyboard = [
        [InlineKeyboardButton("金 砖转 住驻专", callback_data='sports_news')],
        [InlineKeyboardButton(" 砖转 ", callback_data='tech_news')],
        [InlineKeyboardButton(" 砖转 注专爪 ", callback_data='tv_news')]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await update.message.reply_text(text=message, parse_mode='Markdown', disable_web_page_preview=True, reply_markup=reply_markup)

async def sports_news(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    user_id = query.from_user.id
    chat = await context.bot.get_chat(user_id)
    username = chat.username
    logger.info(f"User {user_id} triggered sports_news, username: {username}")
    log_interaction(user_id, "sports_news", username)
    await query.answer()
    
    await query.message.reply_text("驻砖 拽 住驻专...")
    
    sport5_news, sport5_error = scrape_sport5()
    sport1_news, sport1_error = scrape_sport1()
    one_news, one_error = scrape_one()
    
    message = "**住驻专 5**\n"
    if sport5_news:
        for idx, article in enumerate(sport5_news[:3], 1):
            if 'time' in article:
                message += f"{idx}. {article['time']} - [{article['title']}]({article['link']})\n"
            else:
                message += f"{idx}. [{article['title']}]({article['link']})\n"
    else:
        message += " 转 爪 拽\n"
        if sport5_error:
            message += f"**驻专 砖:** {sport5_error}\n"
    
    message += "\n**住驻专 1**\n"
    if sport1_news:
        for idx, article in enumerate(sport1_news[:3], 1):
            if 'time' in article:
                message += f"{idx}. {article['time']} - [{article['title']}]({article['link']})\n"
            else:
                message += f"{idx}. [{article['title']}]({article['link']})\n"
    else:
        message += " 转 爪 拽\n"
        if sport1_error:
            message += f"**驻专 砖:** {sport1_error}\n"
    
    message += "\n**ONE**\n"
    if one_news:
        for idx, article in enumerate(one_news[:3], 1):
            message += f"{idx}. [{article['title']}]({article['link']})\n"
    else:
        message += " 转 爪 拽\n"
        if one_error:
            message += f"**驻专 砖:** {one_error}\n"
    
    keyboard = [[InlineKeyboardButton(" 专 注 专砖", callback_data='latest_news')]]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await query.message.reply_text(text=message, parse_mode='Markdown', disable_web_page_preview=True, reply_markup=reply_markup)

async def tech_news(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    user_id = query.from_user.id
    chat = await context.bot.get_chat(user_id)
    username = chat.username
    logger.info(f"User {user_id} triggered tech_news, username: {username}")
    log_interaction(user_id, "tech_news", username)
    await query.answer()
    
    await query.message.reply_text("驻砖 砖转 ...")
    
    ynet_tech_news, ynet_tech_error = scrape_ynet_tech()
    
    message = "**Ynet Tech**\n"
    if ynet_tech_news:
        for idx, article in enumerate(ynet_tech_news[:3], 1):
            if 'time' in article:
                message += f"{idx}. [{article['time']} - {article['title']}]({article['link']})\n"
            else:
                message += f"{idx}. [{article['title']}]({article['link']})\n"
    else:
        message += " 转 爪 拽\n"
        if ynet_tech_error:
            message += f"**驻专 砖:** {ynet_tech_error}\n"
    
    keyboard = [[InlineKeyboardButton(" 专 注 专砖", callback_data='latest_news')]]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await query.message.reply_text(text=message, parse_mode='Markdown', disable_web_page_preview=True, reply_markup=reply_markup)

async def tv_news(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    user_id = query.from_user.id
    chat = await context.bot.get_chat(user_id)
    username = chat.username
    logger.info(f"User {user_id} triggered tv_news, username: {username}")
    log_interaction(user_id, "tv_news", username)
    await query.answer()
    
    await query.message.reply_text("驻砖 砖转 注专爪 ...")
    
    channel14_news, channel14_error = scrape_channel14()
    
    message = "**砖转 注专爪 **\n\n"
    message += "** 11**: (注专: 驻拽爪 注 )\n"
    message += "**拽砖转 12**: (注专: 驻拽爪 注 )\n"
    message += "**专砖转 13**: (注专: 驻拽爪 注 )\n"
    message += "**注砖 14**:\n"
    if channel14_news:
        for idx, article in enumerate(channel14_news[:3], 1):
            if 'time' in article:
                message += f"{idx}. [{article['time']} - {article['title']}]({article['link']})\n"
            else:
                message += f"{idx}. [{article['title']}]({article['link']})\n"
    else:
        message += " 转 爪 拽\n"
        if channel14_error:
            message += f"**驻专 砖:** {channel14_error}\n"
    
    keyboard = [[InlineKeyboardButton(" 专 注 专砖", callback_data='latest_news')]]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await query.message.reply_text(text=message, parse_mode='Markdown', disable_web_page_preview=True, reply_markup=reply_markup)

async def latest_news(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    user_id = query.from_user.id
    chat = await context.bot.get_chat(user_id)
    username = chat.username
    logger.info(f"User {user_id} triggered latest_news, username: {username}")
    log_interaction(user_id, "latest_news", username)
    await query.answer()
    
    ynet_news = scrape_ynet()
    arutz7_news = scrape_arutz7()
    walla_news = scrape_walla()

    news = {'Ynet': ynet_news, '注专抓 7': arutz7_news, 'Walla': walla_news}
    message = " **拽 专** \n\n"
    for site, articles in news.items():
        message += f"**{site}:**\n"
        if articles:
            for idx, article in enumerate(articles[:3], 1):
                if 'time' in article:
                    message += f"{idx}. [{article['time']} - {article['title']}]({article['link']})\n"
                else:
                    message += f"{idx}. [{article['title']}]({article['link']})\n"
        else:
            message += " 转 注 专注\n"
        message += "\n"
    
    keyboard = [
        [InlineKeyboardButton("金 砖转 住驻专", callback_data='sports_news')],
        [InlineKeyboardButton(" 砖转 ", callback_data='tech_news')],
        [InlineKeyboardButton(" 砖转 注专爪 ", callback_data='tv_news')]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await query.message.reply_text(text=message, parse_mode='Markdown', disable_web_page_preview=True, reply_markup=reply_markup)

# 砖专转 Flask  砖拽砖 驻专
@app.route('/')
def home():
    return "Bot is alive!"

def run_flask():
    port = int(os.environ.get("PORT", 8080))
    app.run(host="0.0.0.0", port=port)

if __name__ == "__main__":
    logger.info("转 转  注 Polling 砖专转 ...")
    
    # 专转 驻
    bot_app.add_handler(CommandHandler("start", start))
    bot_app.add_handler(CommandHandler("latest", latest))
    bot_app.add_handler(CommandHandler("download", download))
    bot_app.add_handler(CallbackQueryHandler(sports_news, pattern='sports_news'))
    bot_app.add_handler(CallbackQueryHandler(tech_news, pattern='tech_news'))
    bot_app.add_handler(CallbackQueryHandler(tv_news, pattern='tv_news'))
    bot_app.add_handler(CallbackQueryHandler(latest_news, pattern='latest_news'))

    # 专爪转 Flask 砖专砖专 驻专
    flask_thread = threading.Thread(target=run_flask, daemon=True)
    flask_thread.start()

    # 转 专爪转 Polling
    bot_app.run_polling(allowed_updates=Update.ALL_TYPES)
