import os
import time
import requests
from bs4 import BeautifulSoup
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, CallbackQueryHandler, ContextTypes
from flask import Flask
import threading
import logging
import asyncio
import json
import gzip
from data_logger import log_interaction, save_to_excel
from sports_scraper import scrape_sport5, scrape_sport1, scrape_one
import signal
from contextlib import contextmanager

# ×”×’×“×¨×ª ×œ×•×’×™× ×’
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

logger.debug("Checking TELEGRAM_TOKEN...")
TOKEN = os.getenv("TELEGRAM_TOKEN")
if not TOKEN:
    logger.error("×©×’×™××”: TELEGRAM_TOKEN ×œ× ××•×’×“×¨! ×”×‘×•×˜ ×œ× ×™×¨×•×¥.")
    exit(1)
logger.info(f"TELEGRAM_TOKEN found: {TOKEN[:5]}... (shortened for security)")

# ×”×’×“×¨×ª API ×©×œ Apify ×¢× ××™××•×ª ×˜×•×§×Ÿ
APIFY_API_TOKEN = os.getenv("APIFY_API_TOKEN")
if not APIFY_API_TOKEN:
    logger.error("×©×’×™××”: APIFY_API_TOKEN ×œ× ××•×’×“×¨! ×× × ×”×’×“×¨ ××•×ª×• ×‘-Render ×ª×—×ª Environment Variables ×¢× ×”×˜×•×§×Ÿ ×”×××™×ª×™ ×-Apify Console.")
    exit(1)
logger.debug(f"APIFY_API_TOKEN length: {len(APIFY_API_TOKEN)} characters (not showing full token for security)")
APIFY_ACTOR_ID = "XjjDkeadhnlDBTU6i"
APIFY_API_URL = "https://api.apify.com/v2"

NEWS_SITES = {
    'ynet': 'https://www.ynet.co.il/news',
    'arutz7': 'https://www.inn.co.il/api/NewAPI/Cat?type=10',
    'walla': 'https://news.walla.co.il/',
    'ynet_tech': 'https://www.ynet.co.il/digital/technews',
    'kan11': 'https://www.kan.org.il/umbraco/surface/NewsFlashSurface/GetNews?currentPageId=1579',
    'kan11_alt': 'https://www.kan.org.il/news-flash',
    'channel14': 'https://www.now14.co.il/feed/'
}

BASE_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.9,he;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'DNT': '1',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Referer': 'https://www.google.com/'
}

app = Flask(__name__)

# × ×§×•×“×ª ×§×¦×” ×‘×¡×™×¡×™×ª ×›×“×™ ×œ×”×’×™×‘ ×œ×‘×§×©×•×ª ×-UptimeRobot
@app.route('/')
def home():
    logger.info("Received GET request at /")
    return "Bot is alive!"

bot_app = Application.builder().token(TOKEN).build()

@contextmanager
def timeout(seconds):
    def handler(signum, frame):
        raise TimeoutError("×¡×§×¨×™×™×¤×™× ×’ × ××©×š ×™×•×ª×¨ ××“×™ ×–××Ÿ")
    signal.signal(signal.SIGALRM, handler)
    signal.alarm(seconds)
    try:
        yield
    finally:
        signal.alarm(0)

def scrape_ynet():
    logger.debug("Scraping Ynet...")
    try:
        response = requests.get(NEWS_SITES['ynet'], headers=BASE_HEADERS)
        soup = BeautifulSoup(response.text, 'html.parser')
        return [{'title': item.text.strip(), 'link': item.find('a')['href']} for item in soup.select('div.slotTitle')[:5]]
    except Exception as e:
        logger.error(f"×©×’×™××” ×‘-Ynet: {e}")
        return []

def scrape_arutz7():
    logger.debug("Scraping Arutz 7...")
    try:
        # ×©×œ×™×—×ª ×”×‘×§×©×” ×¢× timeout
        response = requests.get(NEWS_SITES['arutz7'], headers=BASE_HEADERS, timeout=10)
        logger.debug(f"Arutz 7 API response status: {response.status_code}")
        logger.debug(f"Response headers: {response.headers}")
        logger.debug(f"Raw response content (first 500 bytes): {repr(response.content[:500])}... (truncated)")

        # ×‘×“×™×§×ª ×§×•×“ ×¡×˜×˜×•×¡
        if response.status_code != 200:
            logger.error(f"×§×•×“ ×¡×˜×˜×•×¡ ×œ× ×ª×§×™×Ÿ: {response.status_code}. ×ª×’×•×‘×”: {response.text}")
            return []

        # ×‘×“×™×§×” ×× ×”×ª×’×•×‘×” ×¨×™×§×”
        if not response.content:
            logger.error("×ª×’×•×‘×” ×¨×™×§×” ××”-API ×©×œ ×¢×¨×•×¥ 7")
            return []

        # × ×™×¡×™×•×Ÿ ×œ×¤×¢× ×— ×›-gzip
        content = response.content
        try:
            logger.debug("×× ×¡×” ×œ×¤×¢× ×— ×›-gzip...")
            decompressed_content = gzip.decompress(content)
            data = json.loads(decompressed_content.decode('utf-8'))
            logger.debug("×¤×¢× ×•×— gzip ×”×¦×œ×™×—!")
        except (gzip.BadGzipFile, ValueError) as gzip_err:
            logger.debug(f"×¤×¢× ×•×— gzip × ×›×©×œ: {gzip_err}. ×× ×¡×” ×›-JSON ×¨×’×™×œ...")
            try:
                data = response.json()
            except json.JSONDecodeError as json_err:
                logger.error(f"×©×’×™××” ×‘×¤×¨×™×§×ª JSON: {json_err}. ×ª×’×•×‘×” ×’×•×œ××™×ª: {repr(content[:500])}")
                return []

        # ×‘×“×™×§×ª ××‘× ×” ×”× ×ª×•× ×™×
        items = data.get('Items', []) if isinstance(data, dict) and 'Items' in data else data
        if not items:
            logger.warning("×œ× × ××¦××• ×¤×¨×™×˜×™× ×‘×ª×’×•×‘×ª ×”-API")
            return []

        # ×¢×™×‘×•×“ ×”×¤×¨×™×˜×™×
        return [
            {
                'time': item.get('time', item.get('itemDate', "×œ×œ× ×©×¢×”")[:16].replace('T', ' ')),
                'title': item.get('title', '×œ×œ× ×›×•×ª×¨×ª'),
                'link': item.get('shotedLink', item.get('link', '#'))
            } for item in items[:3]
        ]
    except requests.exceptions.RequestException as req_err:
        logger.error(f"×©×’×™××” ×‘×‘×§×©×” ×œ-API ×©×œ ×¢×¨×•×¥ 7: {req_err}")
        return []
    except Exception as e:
        logger.error(f"×©×’×™××” ×œ× ×¦×¤×•×™×” ×‘×¢×¨×•×¥ 7: {e}")
        return []

def scrape_walla():
    logger.debug("Scraping Walla...")
    try:
        response = requests.get(NEWS_SITES['walla'], headers=BASE_HEADERS)
        soup = BeautifulSoup(response.text, 'html.parser')
        items = soup.select_one('div.top-section-newsflash.no-mobile').select('a') if soup.select_one('div.top-section-newsflash.no-mobile') else []
        results = []
        for item in items:
            title = item.get_text(strip=True)
            if title in ["××‘×–×§×™ ×—×“×©×•×ª", "××‘×–×§×™×"]:
                continue
            if len(title) > 5 and title[2] == ':':
                title = title[:5] + ": " + title[5:]
            link = item['href']
            if not link.startswith('http'):
                link = f"https://news.walla.co.il{link}"
            results.append({'title': title, 'link': link})
        return results[:3]
    except Exception as e:
        logger.error(f"×©×’×™××” ×‘-Walla: {e}")
        return []

def scrape_ynet_tech():
    logger.debug("Scraping Ynet Tech...")
    try:
        response = requests.get(NEWS_SITES['ynet_tech'], headers=BASE_HEADERS, timeout=5)
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.select('div.slotView')[:3]
        results = []
        for idx, article in enumerate(articles):
            title_tag = article.select_one('div.slotTitle a')
            link_tag = title_tag
            time_tag = article.select_one('span.dateView')
            title = title_tag.get_text(strip=True) if title_tag else '×œ×œ× ×›×•×ª×¨×ª'
            link = link_tag['href'] if link_tag else '#'
            article_time = time_tag.get_text(strip=True) if time_tag else '×œ×œ× ×©×¢×”'
            if not link.startswith('http'):
                link = f"https://www.ynet.co.il{link}"
            results.append({'time': article_time, 'title': title, 'link': link})
        return results, None
    except Exception as e:
        logger.error(f"×©×’×™××” ×‘×¡×§×¨×™×¤×™× ×’ Ynet Tech: {str(e)}")
        return [], f"×©×’×™××” ×œ× ×™×“×•×¢×”: {str(e)}"

def scrape_kan11():
    logger.debug("Scraping Kan 11...")
    try:
        with timeout(60):  # ××’×‘×™×œ ×œ-60 ×©× ×™×•×ª
            response = requests.get(NEWS_SITES['kan11'], headers=BASE_HEADERS, timeout=15)
            response.raise_for_status()
            time.sleep(2)  # ×”××ª× ×” ×§×¦×¨×” ×›×“×™ ×œ×”×¤×—×™×ª ×—×©×“
            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.select('div.accordion-item.f-news__item')[:3]
            if not items:
                logger.warning("×œ× × ××¦××• ××‘×–×§×™× ×‘-URL ×”×¨××©×™, ×× ×¡×” URL ×—×œ×•×¤×™")
                response = requests.get(NEWS_SITES['kan11_alt'], headers=BASE_HEADERS, timeout=15)
                response.raise_for_status()
                time.sleep(2)
                soup = BeautifulSoup(response.text, 'html.parser')
                items = soup.select('div.accordion-item.f-news__item')[:3]
                if not items:
                    logger.debug(f"Kan 11 HTML: {response.text[:500]}")
                    return [], "×œ× × ××¦××• ××‘×–×§×™× ×‘-HTML"
            results = []
            for item in items:
                time_tag = item.select_one('div.time')
                title_tag = item.select_one('div.d-flex.flex-grow-1 span')
                link_tag = item.select_one('a.card-link')
                article_time = time_tag.get_text(strip=True) if time_tag else '×œ×œ× ×©×¢×”'
                title = title_tag.get_text(strip=True) if title_tag else '×œ×œ× ×›×•×ª×¨×ª'
                link = link_tag['href'] if link_tag else None
                if link and not link.startswith('http'):
                    link = f"https://www.kan.org.il{link}"
                results.append({'time': article_time, 'title': title, 'link': link})
            logger.info(f"×¡×§×¨×™×¤×™× ×’ ×›××Ÿ 11 ×”×¦×œ×™×—: {len(results)} ××‘×–×§×™×")
            return results, None
    except requests.exceptions.HTTPError as e:
        logger.error(f"×©×’×™××” ×‘-HTTP: {e}")
        return [], f"×©×’×™××” ×‘×©×¨×ª: {e}"
    except TimeoutError:
        logger.error("×¡×§×¨×™×™×¤×™× ×’ ×›××Ÿ 11 × ×›×©×œ: ×œ×§×— ×™×•×ª×¨ ×-60 ×©× ×™×•×ª")
        return [], "×œ×§×— ×™×•×ª×¨ ××“×™ ×–××Ÿ"
    except Exception as e:
        logger.error(f"×©×’×™××” ×‘×¡×§×¨×™×¤×™× ×’ ×›××Ÿ 11: {str(e)}")
        return [], f"×©×’×™××” ×‘×¡×§×¨×™×¤×™× ×’: {str(e)}"

async def run_apify_actor():
    logger.debug("Running Apify Actor...")
    max_retries = 3
    retry_delay = 5  # ×¢×™×›×•×‘ ×©×œ 5 ×©× ×™×•×ª ×‘×™×Ÿ × ×™×¡×™×•× ×•×ª

    for attempt in range(max_retries):
        try:
            url = f"{APIFY_API_URL}/acts/{APIFY_ACTOR_ID}/runs?limit=2&desc=1"  # ××‘×§×© 2 ×¨×™×¦×•×ª
            headers = {
                "Authorization": f"Bearer {APIFY_API_TOKEN}",
                "Content-Type": "application/json"
            }
            response = requests.get(url, headers=headers, timeout=30)
            if response.status_code != 200:
                logger.error(f"Failed to fetch latest Apify Actor run: {response.status_code} - {response.text}")
                response.raise_for_status()
            run_data = response.json()
            
            if not run_data.get('data', {}).get('items'):
                logger.error("No runs found for this Actor.")
                return [], "×œ× × ××¦××• ×¨×™×¦×•×ª ×¢×‘×•×¨ ×”-Actor ×”×–×”. ×•×“× ×©×”-Actor ××•×’×“×¨ ×œ×¨×•×¥ ×›×œ ×©×¢×” ×‘-Apify."

            runs = run_data['data']['items']
            latest_run = runs[0]
            run_id = latest_run['id']
            status = latest_run['status']
            logger.debug(f"Latest run ID: {run_id}, Status: {status}")

            if status == 'RUNNING' and len(runs) > 1:
                logger.warning("Latest run is RUNNING, checking previous run...")
                previous_run = runs[1]
                if previous_run['status'] == 'SUCCEEDED':
                    latest_run = previous_run
                    status = 'SUCCEEDED'
                    logger.info("Using previous successful run.")
                else:
                    if attempt < max_retries - 1:
                        logger.info(f"Waiting {retry_delay} seconds before retrying...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        logger.error("All runs failed or still running after retries.")
                        return [], "×›×œ ×”×¨×™×¦×•×ª × ×›×©×œ×• ××• ×¢×“×™×™×Ÿ ×¨×¦×•×ª."

            if status != 'SUCCEEDED':
                logger.error(f"Latest Actor run did not succeed. Status: {status}")
                return [], f"×”×¨×™×¦×” ×”××—×¨×•× ×” ×©×œ ×”-Actor ×œ× ×”×¦×œ×™×—×”. ×¡×˜×˜×•×¡: {status}"

            dataset_id = latest_run['defaultDatasetId']
            if not dataset_id:
                logger.error("No dataset ID found in the latest run.")
                return [], "×œ× × ××¦× Dataset ID ×¢×‘×•×¨ ×”×¨×™×¦×” ×”××—×¨×•× ×”."

            dataset_url = f"{APIFY_API_URL}/datasets/{dataset_id}/items"
            dataset_response = requests.get(dataset_url, headers=headers, timeout=30)
            if dataset_response.status_code != 200:
                logger.error(f"Failed to fetch dataset items: {dataset_response.status_code} - {dataset_response.text}")
                dataset_response.raise_for_status()
            
            dataset_items = dataset_response.json()
            logger.debug(f"Dataset items: {json.dumps(dataset_items, ensure_ascii=False)[:2000]}... (truncated)")
            if not dataset_items:
                logger.warning("×œ× × ××¦××• ×¤×¨×™×˜×™× ×‘-Dataset")
                return [], "×œ× × ××¦××• ××‘×–×§×™× ×‘-Dataset ×©×œ ×”×¨×™×¦×” ×”××—×¨×•× ×”"

            results = []
            for item in dataset_items[:3]:  # ×¢×“ 3 ×¤×¨×™×˜×™×
                content = item.get('content', '')
                logger.debug(f"Processing dataset item content (raw): {content[:2000]}... (truncated)")
                if not content:
                    logger.warning("Content is empty for this item")
                    continue
                
                soup = BeautifulSoup(content, 'html.parser')
                pre_content = soup.find('pre').text if soup.find('pre') else content
                logger.debug(f"Cleaned pre content: {pre_content[:2000]}... (truncated)")
                
                rss_soup = BeautifulSoup(pre_content, 'lxml')
                items = rss_soup.select('item')[:3]
                if not items:
                    logger.warning("×œ× × ××¦××• ×ª×’×™×•×ª <item> ×‘-RSS")
                    all_tags = [tag.name for tag in rss_soup.find_all()]
                    logger.debug(f"×›×œ ×”×ª×’×™×•×ª ×©× ××¦××• ×‘-RSS: {all_tags}")
                    logger.debug(f"Full RSS structure: {rss_soup.prettify()[:2000]}... (truncated)")
                    continue

                for rss_item in items:
                    title = rss_item.find('title')
                    title = title.get_text(strip=True) if title else '×œ×œ× ×›×•×ª×¨×ª'
                    
                    # ×—×™×œ×•×¥ ×”×§×™×©×•×¨ ××ª×’×™×ª <link>
                    link = None
                    link_tag = rss_item.find('link')
                    if link_tag and link_tag.string:
                        link = link_tag.string.strip()
                    logger.debug(f"Extracted link for item '{title}': {link}")
                    
                    # ×× ×œ× × ××¦× ×§×™×©×•×¨ ×‘×ª×’×™×ª <link>, × × ×¡×” ××ª <guid>
                    if not link:
                        guid_tag = rss_item.find('guid')
                        if guid_tag and guid_tag.string:
                            link = guid_tag.string.strip()
                        logger.debug(f"Extracted link from guid for item '{title}': {link}")
                    
                    # ×—×™×œ×•×¥ ×–××Ÿ ××ª×’×™×ª <pubDate>
                    pub_date = rss_item.find('pubdate')
                    if pub_date and pub_date.string:
                        pub_date = pub_date.string.strip()
                        try:
                            pub_date = pub_date.split('+')[0].strip()
                            pub_date = ' '.join(pub_date.split()[1:4])
                        except Exception as e:
                            logger.debug(f"Error formatting pubDate for item '{title}': {e}")
                    else:
                        pub_date = '×œ×œ× ×©×¢×”'
                    
                    # ×× ×œ× × ××¦× ×–××Ÿ ×‘-<pubDate>, × × ×¡×” ×ª×’×™×ª ×—×œ×•×¤×™×ª ×›××• <dc:date>
                    if pub_date == '×œ×œ× ×©×¢×”':
                        date_tag = rss_item.find('dc:date')
                        pub_date = date_tag.string.strip() if date_tag and date_tag.string else '×œ×œ× ×©×¢×”'
                        if pub_date != '×œ×œ× ×©×¢×”':
                            try:
                                pub_date = pub_date.split('T')[0] + ' ' + pub_date.split('T')[1].split('+')[0]
                            except Exception as e:
                                logger.debug(f"Error formatting dc:date for item '{title}': {e}")
                    
                    results.append({'time': pub_date, 'title': title, 'link': link})

            if not results:
                logger.warning("×œ× × ××¦××• ××‘×–×§×™× ×ª×§×™× ×™× ×œ××—×¨ ×¢×™×‘×•×“")
                return [], "×œ× × ××¦××• ××‘×–×§×™× ×ª×§×™× ×™× ×œ××—×¨ ×¢×™×‘×•×“"

            logger.info(f"×©××™×‘×” ××¢×¨×•×¥ 14 ×“×¨×š Apify ×”×¦×œ×™×—×”: {len(results)} ××‘×–×§×™×")
            return results, None

        except Exception as e:
            logger.error(f"×©×’×™××” ×‘×©××™×‘×ª ×ª×•×¦××•×ª ×”-Actor ×-Apify: {str(e)}")
            if attempt < max_retries - 1:
                logger.info(f"Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
                continue
            return [], f"×©×’×™××” ×‘×©××™×‘×” ×“×¨×š Apify ×œ××—×¨ {max_retries} × ×™×¡×™×•× ×•×ª: {str(e)}"

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info("Received /start command")
    user_id = update.message.from_user.id
    chat = await context.bot.get_chat(user_id)
    username = chat.username
    logger.debug(f"User {user_id} sent /start, username: {username}")
    log_interaction(user_id, "/start", username)
    await update.message.reply_text("×‘×¨×•×š ×”×‘×! ×”×©×ª××© ×‘-/latest ×œ××‘×–×§×™×.")

async def download(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info("Received /download command")
    user_id = update.message.from_user.id
    chat = await context.bot.get_chat(user_id)
    username = chat.username
    logger.debug(f"User {user_id} sent /download, username: {username}")
    log_interaction(user_id, "/download", username)
    SECRET_PASSWORD = os.getenv("DOWNLOAD_PASSWORD")

    if not SECRET_PASSWORD:
        await update.message.reply_text("×©×’×™××”: ×”×¡×™×¡××” ×œ× ××•×’×“×¨×ª ×‘×©×¨×ª!")
        return
    
    if not context.args or context.args[0] != SECRET_PASSWORD:
        await update.message.reply_text("×¡×™×¡××” ×©×’×•×™×”! ××™×Ÿ ×’×™×©×”.")
        return
    
    try:
        filename = save_to_excel()
        if not os.path.exists(filename):
            await update.message.reply_text("×©×’×™××”: ×”×§×•×‘×¥ ×œ× × ×•×¦×¨!")
            return
        with open(filename, 'rb') as file:
            await update.message.reply_text("×”× ×” ×”× ×ª×•× ×™× ×©×œ×š!")
            await update.message.reply_document(document=file, filename="bot_usage.xlsx")
        os.remove(filename)
    except Exception as e:
        logger.error(f"×©×’×™××” ×‘×©×œ×™×—×ª ×”×§×•×‘×¥: {e}")
        await update.message.reply_text(f"×©×’×™××” ×‘×”×•×¨×“×”: {str(e)}")

async def latest(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info("Received /latest command")
    user_id = update.message.from_user.id
    chat = await context.bot.get_chat(user_id)
    username = chat.username
    logger.debug(f"User {user_id} sent /latest, username: {username}")
    log_interaction(user_id, "/latest", username)
    await update.message.reply_text("××—×¤×© ××‘×–×§×™×...")
    ynet_news = scrape_ynet()
    arutz7_news = scrape_arutz7()
    walla_news = scrape_walla()

    news = {'Ynet': ynet_news, '×¢×¨×•×¥ 7': arutz7_news, 'Walla': walla_news}
    message = "ğŸ“° **×”××‘×–×§×™× ×”××—×¨×•× ×™×** ğŸ“°\n\n"
    for site, articles in news.items():
        message += f"**{site}:**\n"
        if articles:
            for idx, article in enumerate(articles[:3], 1):
                if 'time' in article:
                    full_text = f"{article['time']} - {article['title']}"
                    message += f"{idx}. [{full_text}]({article['link']})\n"
                else:
                    message += f"{idx}. [{article['title']}]({article['link']})\n"
        else:
            message += "×œ× × ×™×ª×Ÿ ×œ×˜×¢×•×Ÿ ×›×¨×’×¢\n"
        message += "\n"
    
    keyboard = [
        [InlineKeyboardButton("âš½ğŸ€ ×—×“×©×•×ª ×¡×¤×•×¨×˜", callback_data='sports_news')],
        [InlineKeyboardButton("ğŸ’» ×—×“×©×•×ª ×˜×›× ×•×œ×•×’×™×”", callback_data='tech_news')],
        [InlineKeyboardButton("ğŸ“º ×—×“×©×•×ª ××¢×¨×•×¦×™ ×˜×œ×•×•×™×–×™×”", callback_data='tv_news')]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await update.message.reply_text(text=message, parse_mode='Markdown', disable_web_page_preview=True, reply_markup=reply_markup)

async def sports_news(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info("Received sports_news callback")
    user_id = update.callback_query.from_user.id
    chat = await context.bot.get_chat(user_id)
    username = chat.username
    logger.debug(f"User {user_id} clicked sports_news, username: {username}")
    log_interaction(user_id, "sports_news", username)
    sport5_news = scrape_sport5()
    sport1_news = scrape_sport1()
    one_news = scrape_one()

    message = "âš½ **×—×“×©×•×ª ×¡×¤×•×¨×˜** âš½\n\n"
    news_sources = {'Sport5': sport5_news, 'Sport1': sport1_news, 'One': one_news}
    for source, articles in news_sources.items():
        message += f"**{source}:**\n"
        if articles:
            for idx, article in enumerate(articles[:3], 1):
                message += f"{idx}. [{article['title']}]({article['link']})\n"
        else:
            message += "×œ× × ×™×ª×Ÿ ×œ×˜×¢×•×Ÿ ×›×¨×’×¢\n"
        message += "\n"
    
    await update.callback_query.message.edit_text(text=message, parse_mode='Markdown', disable_web_page_preview=True)

async def tech_news(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info("Received tech_news callback")
    user_id = update.callback_query.from_user.id
    chat = await context.bot.get_chat(user_id)
    username = chat.username
    logger.debug(f"User {user_id} clicked tech_news, username: {username}")
    log_interaction(user_id, "tech_news", username)
    ynet_tech_news, error = scrape_ynet_tech()
    if error:
        await update.callback_query.message.edit_text(f"×©×’×™××”: {error}")
        return
    
    message = "ğŸ’» **×—×“×©×•×ª ×˜×›× ×•×œ×•×’×™×”** ğŸ’»\n\n"
    message += "**Ynet Tech:**\n"
    if ynet_tech_news:
        for idx, article in enumerate(ynet_tech_news[:3], 1):
            full_text = f"{article['time']} - {article['title']}"
            message += f"{idx}. [{full_text}]({article['link']})\n"
    else:
        message += "×œ× × ×™×ª×Ÿ ×œ×˜×¢×•×Ÿ ×›×¨×’×¢\n"
    
    await update.callback_query.message.edit_text(text=message, parse_mode='Markdown', disable_web_page_preview=True)

async def tv_news(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info("Received tv_news callback")
    user_id = update.callback_query.from_user.id
    chat = await context.bot.get_chat(user_id)
    username = chat.username
    logger.debug(f"User {user_id} clicked tv_news, username: {username}")
    log_interaction(user_id, "tv_news", username)
    kan11_news, error = scrape_kan11()
    channel14_news, error_apify = await run_apify_actor()

    message = "ğŸ“º **×—×“×©×•×ª ××¢×¨×•×¦×™ ×˜×œ×•×•×™×–×™×”** ğŸ“º\n\n"
    news_sources = {'Kan 11': kan11_news, '×¢×¨×•×¥ 14': channel14_news}
    for source, articles in news_sources.items():
        message += f"**{source}:**\n"
        if articles:
            for idx, article in enumerate(articles[:3], 1):
                full_text = f"{article['time']} - {article['title']}"
                message += f"{idx}. [{full_text}]({article['link']})\n"
        else:
            message += "×œ× × ×™×ª×Ÿ ×œ×˜×¢×•×Ÿ ×›×¨×’×¢\n"
        message += "\n"
    
    await update.callback_query.message.edit_text(text=message, parse_mode='Markdown', disable_web_page_preview=True)

def run_bot():
    logger.info("Starting bot...")
    bot_app.add_handler(CommandHandler("start", start))
    bot_app.add_handler(CommandHandler("download", download))
    bot_app.add_handler(CommandHandler("latest", latest))
    bot_app.add_handler(CallbackQueryHandler(sports_news, pattern='^sports_news$'))
    bot_app.add_handler(CallbackQueryHandler(tech_news, pattern='^tech_news$'))
    bot_app.add_handler(CallbackQueryHandler(tv_news, pattern='^tv_news$'))
    
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(bot_app.run_polling(allowed_updates=Update.ALL_TYPES))
    except Exception as e:
        logger.error(f"×©×’×™××” ×‘×”×¨×¦×ª ×”×‘×•×˜: {e}")
    finally:
        loop.close()

def run_flask():
    logger.info("Starting Flask server...")
    app.run(host='0.0.0.0', port=5000)

if __name__ == "__main__":
    bot_thread = threading.Thread(target=run_bot, daemon=True)
    bot_thread.start()
    run_flask()
